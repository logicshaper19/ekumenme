"""
Realistic Performance Test - Simulates Real API Latency

Tests caching performance with realistic API delays to show
actual production speedup (not just mock data speedup).
"""

import asyncio
import time
import sys
from pathlib import Path
from unittest.mock import patch, AsyncMock

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from app.tools.weather_agent.get_weather_data_tool_enhanced import (
    get_weather_data_tool_enhanced,
    weather_service,
    EnhancedWeatherService
)
from app.core.cache import clear_cache, get_cache_stats


async def test_realistic_api_latency():
    """
    Test caching with realistic API latency
    
    Simulates:
    - Real weather API: ~500ms response time
    - Cache hit: ~3ms response time
    - Expected speedup: ~99%
    """
    print("\n" + "="*80)
    print("REALISTIC PERFORMANCE TEST - With API Latency Simulation")
    print("="*80)
    
    # Clear cache first
    clear_cache(category="weather")
    print("\nüßπ Cache cleared")
    
    # Store original method
    original_method = weather_service._get_weather_forecast_cached
    
    # Create a wrapper that adds realistic API delay
    async def slow_weather_fetch(self, location, days, coordinates=None, use_real_api=True):
        """Simulate realistic API latency (500ms)"""
        print(f"   üåê Simulating API call with 500ms latency...")
        await asyncio.sleep(0.5)  # Simulate 500ms API call
        
        # Call original method (which returns mock data)
        return await original_method(location, days, coordinates, use_real_api)
    
    # Patch the method
    with patch.object(
        EnhancedWeatherService,
        '_get_weather_forecast_cached',
        slow_weather_fetch
    ):
        # Test 1: First call (cache miss + API latency)
        print("\nüìû Test 1: First call (cache miss + API latency)")
        start = time.time()
        result1 = await get_weather_data_tool_enhanced.ainvoke({
            "location": "Normandie",
            "days": 7,
            "use_real_api": False
        })
        time1 = time.time() - start
        print(f"   ‚è±Ô∏è  Time: {time1:.3f}s")
        print(f"   üìä Result length: {len(result1)} chars")
        
        # Test 2: Second call (cache hit, no API call)
        print("\nüìû Test 2: Second call (cache hit, no API latency)")
        start = time.time()
        result2 = await get_weather_data_tool_enhanced.ainvoke({
            "location": "Normandie",
            "days": 7,
            "use_real_api": False
        })
        time2 = time.time() - start
        print(f"   ‚è±Ô∏è  Time: {time2:.3f}s")
        print(f"   üìä Result length: {len(result2)} chars")
        
        # Verify results are identical
        if result1 == result2:
            print("   ‚úÖ Results identical (cache working)")
        else:
            print("   ‚ùå Results differ (cache not working)")
            return False
        
        # Calculate speedup
        speedup_percent = (1 - time2 / time1) * 100
        speedup_factor = time1 / time2
        
        print(f"\nüìä Performance Analysis:")
        print(f"   First call (uncached):  {time1:.3f}s")
        print(f"   Second call (cached):   {time2:.3f}s")
        print(f"   Speedup:                {speedup_percent:.1f}%")
        print(f"   Speed factor:           {speedup_factor:.1f}x faster")
        
        # Expected: ~99% speedup (500ms -> 3ms)
        if speedup_percent > 90:
            print(f"\n‚úÖ EXCELLENT: {speedup_percent:.1f}% speedup (expected ~99%)")
        elif speedup_percent > 70:
            print(f"\n‚úÖ GOOD: {speedup_percent:.1f}% speedup")
        else:
            print(f"\n‚ö†Ô∏è LOWER THAN EXPECTED: {speedup_percent:.1f}% speedup")
        
        # Cache stats
        stats = get_cache_stats()
        print(f"\nüìä Cache Stats:")
        print(f"   Redis available: {stats['redis_available']}")
        print(f"   Total memory items: {stats['total_memory_items']}")
        if 'memory_caches' in stats:
            for category, cache_stats in stats['memory_caches'].items():
                if cache_stats['size'] > 0:
                    print(f"   {category}: {cache_stats['size']}/{cache_stats['maxsize']} items ({cache_stats['utilization']:.1f}%)")
        
        return True


async def test_multiple_locations():
    """
    Test caching with multiple locations
    
    Shows:
    - Cache miss for new location
    - Cache hit for repeated location
    - Per-location caching
    """
    print("\n" + "="*80)
    print("MULTI-LOCATION CACHING TEST")
    print("="*80)
    
    clear_cache(category="weather")
    print("\nüßπ Cache cleared")
    
    locations = ["Normandie", "Paris", "Lyon", "Normandie", "Paris"]
    
    # Store original method
    original_method = weather_service._get_weather_forecast_cached
    
    # Create a wrapper that adds realistic API delay
    async def slow_weather_fetch(self, location, days, coordinates=None, use_real_api=True):
        """Simulate realistic API latency (500ms)"""
        await asyncio.sleep(0.5)  # Simulate 500ms API call
        return await original_method(location, days, coordinates, use_real_api)
    
    with patch.object(
        EnhancedWeatherService,
        '_get_weather_forecast_cached',
        slow_weather_fetch
    ):
        times = []
        cache_hits = 0
        cache_misses = 0
        
        for i, location in enumerate(locations, 1):
            print(f"\nüìû Request {i}: {location}")
            start = time.time()
            result = await get_weather_data_tool_enhanced.ainvoke({
                "location": location,
                "days": 7,
                "use_real_api": False
            })
            elapsed = time.time() - start
            times.append(elapsed)
            
            # Determine if cache hit or miss based on time
            if elapsed > 0.1:  # > 100ms = cache miss (API call)
                print(f"   ‚è±Ô∏è  Time: {elapsed:.3f}s (‚ùå cache miss)")
                cache_misses += 1
            else:
                print(f"   ‚è±Ô∏è  Time: {elapsed:.3f}s (‚úÖ cache hit)")
                cache_hits += 1
        
        print(f"\nüìä Summary:")
        print(f"   Total requests: {len(locations)}")
        print(f"   Cache hits: {cache_hits}")
        print(f"   Cache misses: {cache_misses}")
        print(f"   Hit rate: {cache_hits / len(locations) * 100:.1f}%")
        print(f"   Avg time (all): {sum(times) / len(times):.3f}s")
        print(f"   Total time: {sum(times):.3f}s")
        
        # Calculate time saved
        time_without_cache = len(locations) * 0.5  # All API calls
        time_with_cache = sum(times)
        time_saved = time_without_cache - time_with_cache
        
        print(f"\nüí∞ Time Savings:")
        print(f"   Without cache: {time_without_cache:.3f}s (all API calls)")
        print(f"   With cache: {time_with_cache:.3f}s")
        print(f"   Time saved: {time_saved:.3f}s ({time_saved / time_without_cache * 100:.1f}%)")
        
        return True


async def test_concurrent_requests():
    """
    Test caching with concurrent requests
    
    Shows:
    - Multiple concurrent requests for same location
    - Cache prevents duplicate API calls
    """
    print("\n" + "="*80)
    print("CONCURRENT REQUESTS TEST")
    print("="*80)
    
    clear_cache(category="weather")
    print("\nüßπ Cache cleared")
    
    # Store original method
    original_method = weather_service._get_weather_forecast_cached
    api_calls = []
    
    # Create a wrapper that tracks API calls
    async def tracked_weather_fetch(self, location, days, coordinates=None, use_real_api=True):
        """Track API calls and add latency"""
        api_calls.append({"location": location, "days": days, "time": time.time()})
        print(f"   üåê API call #{len(api_calls)}: {location}")
        await asyncio.sleep(0.5)  # Simulate 500ms API call
        return await original_method(location, days, coordinates, use_real_api)
    
    with patch.object(
        EnhancedWeatherService,
        '_get_weather_forecast_cached',
        tracked_weather_fetch
    ):
        print("\nüìû Launching 5 concurrent requests for 'Normandie'...")
        start = time.time()
        
        # Launch 5 concurrent requests for same location
        tasks = [
            get_weather_data_tool_enhanced.ainvoke({
                "location": "Normandie",
                "days": 7,
                "use_real_api": False
            })
            for _ in range(5)
        ]
        
        results = await asyncio.gather(*tasks)
        elapsed = time.time() - start
        
        print(f"\nüìä Results:")
        print(f"   Total requests: 5")
        print(f"   API calls made: {len(api_calls)}")
        print(f"   Total time: {elapsed:.3f}s")
        print(f"   Expected (no cache): ~2.5s (5 √ó 500ms)")
        print(f"   Expected (with cache): ~0.5s (1 √ó 500ms)")
        
        # Note: Due to async nature, first request might complete before others start
        # So we might see 1-5 API calls depending on timing
        if len(api_calls) == 1:
            print(f"\n‚úÖ PERFECT: Only 1 API call (cache prevented 4 duplicate calls)")
        elif len(api_calls) < 5:
            print(f"\n‚úÖ GOOD: {len(api_calls)} API calls (cache prevented {5 - len(api_calls)} duplicate calls)")
        else:
            print(f"\n‚ö†Ô∏è NO CACHING: All 5 requests hit API")
        
        return True


async def main():
    """Run all realistic performance tests"""
    print("\n" + "="*80)
    print("REALISTIC PERFORMANCE TEST SUITE")
    print("Simulates real API latency to show actual production speedup")
    print("="*80)
    
    tests = [
        ("API Latency Simulation", test_realistic_api_latency),
        ("Multi-Location Caching", test_multiple_locations),
        ("Concurrent Requests", test_concurrent_requests),
    ]
    
    results = []
    
    for name, test_func in tests:
        try:
            result = await test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\n‚ùå TEST FAILED with exception: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))
    
    # Summary
    print("\n" + "="*80)
    print("TEST SUMMARY")
    print("="*80)
    
    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {name}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    print(f"\nüìä Overall: {passed}/{total} tests passed ({passed/total*100:.0f}%)")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED!")
        print("\nüí° Key Takeaway:")
        print("   With realistic API latency (500ms), caching provides ~99% speedup")
        print("   This is MUCH better than the 49.5% shown in mock data tests")
    else:
        print(f"\n‚ö†Ô∏è {total - passed} test(s) failed")
    
    return passed == total


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)

